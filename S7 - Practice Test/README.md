# Section 7: Practice Test
---
- For a clasification task, instead of random weight initializations in a NN, we set all weight to zero. What happens?

- What if we use a learning rate that's too large?

- If you increase the number of hidden layers in a Multi-Layer Perceptron, the classification error of test data will always decrease. (T/F)?

- The differernce between DL and ML algorithms is taht there is no need of feature engineering in ML, whereas, it is recommended to do it first and then applly DL. (T/F)?

- Backpropagation works by first calculating the gradient of ___ and then propagating it backwards.